{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchinfo import summary\n\nclass BasicUnit(nn.Module):\n    def __init__(self, channels: int, dropout: float):\n        super(BasicUnit, self).__init__()\n        self.block = nn.Sequential(OrderedDict([\n            (\"0_normalization\", nn.BatchNorm2d(channels)),\n            (\"1_activation\", nn.ReLU(inplace=True)),\n            (\"2_convolution\", nn.Conv2d(channels, channels, (3, 3), stride=1, padding=1, bias=False)),\n            (\"3_normalization\", nn.BatchNorm2d(channels)),\n            (\"4_activation\", nn.ReLU(inplace=True)),\n            (\"5_dropout\", nn.Dropout(dropout, inplace=True)),\n            (\"6_convolution\", nn.Conv2d(channels, channels, (3, 3), stride=1, padding=1, bias=False)),\n        ]))\n\n    def forward(self, x):\n        return x + self.block(x)\n\n\nclass DownsampleUnit(nn.Module):\n    def __init__(self, in_channels: int, out_channels: int, stride: int, dropout: float):\n        super(DownsampleUnit, self).__init__()\n        self.norm_act = nn.Sequential(OrderedDict([\n            (\"0_normalization\", nn.BatchNorm2d(in_channels)),\n            (\"1_activation\", nn.ReLU(inplace=True)),\n        ]))\n        self.block = nn.Sequential(OrderedDict([\n            (\"0_convolution\", nn.Conv2d(in_channels, out_channels, (3, 3), stride=stride, padding=1, bias=False)),\n            (\"1_normalization\", nn.BatchNorm2d(out_channels)),\n            (\"2_activation\", nn.ReLU(inplace=True)),\n            (\"3_dropout\", nn.Dropout(dropout, inplace=True)),\n            (\"4_convolution\", nn.Conv2d(out_channels, out_channels, (3, 3), stride=1, padding=1, bias=False)),\n        ]))\n        self.downsample = nn.Conv2d(in_channels, out_channels, (1, 1), stride=stride, padding=0, bias=False)\n\n    def forward(self, x):\n        x = self.norm_act(x)\n        return self.block(x) + self.downsample(x)\n\n\nclass Block(nn.Module):\n    def __init__(self, in_channels: int, out_channels: int, stride: int, depth: int, dropout: float):\n        super(Block, self).__init__()\n        self.block = nn.Sequential(\n            DownsampleUnit(in_channels, out_channels, stride, dropout),\n            *(BasicUnit(out_channels, dropout) for _ in range(depth))\n        )\n\n    def forward(self, x):\n        return self.block(x)\n\n\nclass WideResNet(nn.Module):\n    def __init__(self, depth: int, width_factor: int, dropout: float, in_channels: int, labels: int):\n        super(WideResNet, self).__init__()\n\n        self.filters = [16, 1 * 16 * width_factor, 2 * 16 * width_factor, 4 * 16 * width_factor]\n        self.block_depth = (depth - 4) // (3 * 2)\n        \n        self.conv1 = nn.Conv2d(in_channels, self.filters[0], (3, 3), stride=1, padding=1, bias=False)\n        self.block1 = Block(self.filters[0], self.filters[1], 1, self.block_depth, dropout)\n        self.block2 = Block(self.filters[1], self.filters[2], 2, self.block_depth, dropout)\n        self.block3 = Block(self.filters[2], self.filters[3], 2, self.block_depth, dropout)\n        self.batchnorm = nn.BatchNorm2d(self.filters[3])\n        self.relu = nn.ReLU(inplace=True)\n        self.avgpool = nn.AvgPool2d(kernel_size=8)\n        self.flatten = nn.Flatten()\n        self.linear = nn.Linear(in_features=self.filters[3], out_features=labels)\n        \n\n    def _initialize(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight.data, mode=\"fan_in\", nonlinearity=\"relu\")\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                m.weight.data.zero_()\n                m.bias.data.zero_()\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.block1(x)\n        x = self.block2(x)\n        x = self.block3(x)\n        x = self.batchnorm(x)\n        x = self.relu(x)\n        x = self.avgpool(x)\n        x = self.flatten(x)\n        x = self.linear(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-04-24T12:03:04.940606Z","iopub.execute_input":"2023-04-24T12:03:04.941080Z","iopub.status.idle":"2023-04-24T12:03:04.975634Z","shell.execute_reply.started":"2023-04-24T12:03:04.941037Z","shell.execute_reply":"2023-04-24T12:03:04.974253Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def model_test(input_channle=3, image_heigth=224, image_width=224,num_classes=10,batch_size=32):\n    depth = 16\n    width = 8\n    dropout=0.2\n    model = WideResNet(depth, width, dropout, in_channels=3, labels=num_classes)\n    print(model) \n    \nmodel_test()","metadata":{"execution":{"iopub.status.busy":"2023-04-24T12:03:42.748274Z","iopub.execute_input":"2023-04-24T12:03:42.748651Z","iopub.status.idle":"2023-04-24T12:03:42.921756Z","shell.execute_reply.started":"2023-04-24T12:03:42.748620Z","shell.execute_reply":"2023-04-24T12:03:42.920134Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"WideResNet(\n  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (block1): Block(\n    (block): Sequential(\n      (0): DownsampleUnit(\n        (norm_act): Sequential(\n          (0_normalization): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (1_activation): ReLU(inplace=True)\n        )\n        (block): Sequential(\n          (0_convolution): Conv2d(16, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (1_normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2_activation): ReLU(inplace=True)\n          (3_dropout): Dropout(p=0.2, inplace=True)\n          (4_convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (downsample): Conv2d(16, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      )\n      (1): BasicUnit(\n        (block): Sequential(\n          (0_normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (1_activation): ReLU(inplace=True)\n          (2_convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (3_normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (4_activation): ReLU(inplace=True)\n          (5_dropout): Dropout(p=0.2, inplace=True)\n          (6_convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n      )\n      (2): BasicUnit(\n        (block): Sequential(\n          (0_normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (1_activation): ReLU(inplace=True)\n          (2_convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (3_normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (4_activation): ReLU(inplace=True)\n          (5_dropout): Dropout(p=0.2, inplace=True)\n          (6_convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n      )\n    )\n  )\n  (block2): Block(\n    (block): Sequential(\n      (0): DownsampleUnit(\n        (norm_act): Sequential(\n          (0_normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (1_activation): ReLU(inplace=True)\n        )\n        (block): Sequential(\n          (0_convolution): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (1_normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2_activation): ReLU(inplace=True)\n          (3_dropout): Dropout(p=0.2, inplace=True)\n          (4_convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (downsample): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n      )\n      (1): BasicUnit(\n        (block): Sequential(\n          (0_normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (1_activation): ReLU(inplace=True)\n          (2_convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (3_normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (4_activation): ReLU(inplace=True)\n          (5_dropout): Dropout(p=0.2, inplace=True)\n          (6_convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n      )\n      (2): BasicUnit(\n        (block): Sequential(\n          (0_normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (1_activation): ReLU(inplace=True)\n          (2_convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (3_normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (4_activation): ReLU(inplace=True)\n          (5_dropout): Dropout(p=0.2, inplace=True)\n          (6_convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n      )\n    )\n  )\n  (block3): Block(\n    (block): Sequential(\n      (0): DownsampleUnit(\n        (norm_act): Sequential(\n          (0_normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (1_activation): ReLU(inplace=True)\n        )\n        (block): Sequential(\n          (0_convolution): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n          (1_normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2_activation): ReLU(inplace=True)\n          (3_dropout): Dropout(p=0.2, inplace=True)\n          (4_convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n        (downsample): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n      )\n      (1): BasicUnit(\n        (block): Sequential(\n          (0_normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (1_activation): ReLU(inplace=True)\n          (2_convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (3_normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (4_activation): ReLU(inplace=True)\n          (5_dropout): Dropout(p=0.2, inplace=True)\n          (6_convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n      )\n      (2): BasicUnit(\n        (block): Sequential(\n          (0_normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (1_activation): ReLU(inplace=True)\n          (2_convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n          (3_normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (4_activation): ReLU(inplace=True)\n          (5_dropout): Dropout(p=0.2, inplace=True)\n          (6_convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        )\n      )\n    )\n  )\n  (batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)\n  (flatten): Flatten(start_dim=1, end_dim=-1)\n  (linear): Linear(in_features=512, out_features=10, bias=True)\n)\n","output_type":"stream"}]}]}