{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import Libraries\n\nimport os\nimport shutil\nfrom copy import deepcopy\nimport time\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torchvision import datasets, transforms\nfrom torch.utils.data.sampler import SubsetRandomSampler\nimport argparse\nimport torch.backends.cudnn as cudnn\nimport matplotlib.pyplot as plt\nglobal args, best_prec\nfrom torchmetrics.functional import r2_score, mean_absolute_error\nfrom torchmetrics.functional.classification import multiclass_specificity, \\\n    multiclass_recall, multiclass_accuracy, multiclass_precision, multiclass_f1_score","metadata":{"execution":{"iopub.status.busy":"2023-02-04T19:53:01.945442Z","iopub.execute_input":"2023-02-04T19:53:01.947807Z","iopub.status.idle":"2023-02-04T19:53:04.574161Z","shell.execute_reply.started":"2023-02-04T19:53:01.947770Z","shell.execute_reply":"2023-02-04T19:53:04.572846Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Alex Net Model\nclass AlexNet(nn.Module):\n    def __init__(self, input_channel=3, num_classes=10):\n        super(AlexNet, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(input_channel, 96, kernel_size=11, stride=4, padding=2),\n            nn.ReLU(),\n            nn.LocalResponseNorm(size=5, alpha=0.0001, beta=0.75, k=2),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            nn.Conv2d(96, 256, kernel_size=5, padding=2),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            nn.Conv2d(256, 384, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(384, 384, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n        )\n\n        self.classifier = nn.Sequential(\n            nn.Dropout(),\n            nn.Linear(256 * 6 * 6, 4096),\n            nn.ReLU(),\n            nn.Dropout(),\n            nn.Linear(4096, 4096),\n            nn.ReLU(),\n            nn.Linear(4096, num_classes),\n            nn.Softmax()\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-02-04T19:53:04.576641Z","iopub.execute_input":"2023-02-04T19:53:04.576976Z","iopub.status.idle":"2023-02-04T19:53:04.588690Z","shell.execute_reply.started":"2023-02-04T19:53:04.576937Z","shell.execute_reply":"2023-02-04T19:53:04.587744Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Utility functions\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n        \ndef save_checkpoint(state, is_best, fdir):\n    ''' Save checkpoint of model'''\n    filepath = os.path.join(fdir, 'checkpoint.pth')\n    torch.save(state, filepath)\n    if is_best:\n        shutil.copyfile(filepath, os.path.join(fdir, 'model_best.pth.tar'))\n\n\ndef accuracy(output, target, topk=(1,)):\n    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n    maxk = max(topk)\n    batch_size = target.size(0)\n\n    _, pred = output.topk(maxk, 1, True, True)\n    pred = pred.t()\n    correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n    res = []\n    for k in topk:\n        correct_k = correct[:k].reshape(-1).float().sum(0)\n        res.append(correct_k.mul_(100.0 / batch_size))\n    return res\n\ndef torch_metric(y_pred, y_true, num_classes):\n    recall = multiclass_recall(y_pred, y_true, num_classes=num_classes, average='micro')\n    precision = multiclass_precision(y_pred, y_true, num_classes=num_classes, average='micro')\n    f1_score = multiclass_f1_score(y_pred, y_true, num_classes=num_classes, average='micro')\n    accuracy = multiclass_accuracy(y_pred, y_true, num_classes=num_classes, average='micro')\n    specificity = multiclass_specificity(y_pred, y_true, num_classes=num_classes, average='micro')\n    r2_score_value = r2_score(y_pred, y_true)\n    Mean_absolute_error = mean_absolute_error(y_pred, y_true)\n    return f1_score, recall, precision, Mean_absolute_error, r2_score_value, accuracy, specificity\n\ndef output_results(data_in_loader, best_val_model, num_classes, data_name='train'):\n    correct = AverageMeter()\n    f1_score_value = AverageMeter()\n    recall_value = AverageMeter()\n    precision_value = AverageMeter()\n    mean_absolute_error_value = AverageMeter()\n    r2_score_value = AverageMeter()\n    accuracy_value = AverageMeter()\n    top1 = AverageMeter()\n    top5 = AverageMeter()\n    losses = AverageMeter()\n    specificity_value = AverageMeter()\n    model.load_state_dict(best_val_model)\n    model.eval()\n    with torch.no_grad():\n        for inputs, target in data_in_loader:\n            inputs, target = inputs.cuda(), target.cuda()\n            outputs = model(inputs)\n            loss = criterion(outputs, target)\n            prec = accuracy(outputs, target)[0]\n            prec5 = accuracy(outputs, target, (5,))[0]\n            losses.update(loss.item(), inputs.size(0))\n            top1.update(prec.item(), inputs.size(0))\n            top5.update(prec5.item(), inputs.size(0))\n\n            # out = torch.argmax(out.cpu(), dim=1)\n            # y_pred = out.cpu()\n            # y_true = target.cpu()\n            out = torch.argmax(outputs, dim=1)\n            y_pred = out\n            y_true = target\n            out_f1_score, out_recall, out_precision, out_mean_absolute_error, out_r2_score, \\\n            out_accuracy, out_specificity = torch_metric(y_true, y_pred, num_classes)\n\n            f1_score_value.update(out_f1_score.item(), inputs.size(0))\n            recall_value.update(out_recall.item(), inputs.size(0))\n            precision_value.update(out_precision.item(), inputs.size(0))\n            f1_score_value.update(out_f1_score.item(), inputs.size(0))\n            accuracy_value.update(out_accuracy.item(), inputs.size(0))\n            specificity_value.update(out_specificity.item(), inputs.size(0))\n            r2_score_value.update(out_r2_score.item(), inputs.size(0))\n\n    results = {\n        'Top One Accuracy': top1.avg,\n        'Top Five Accuracy': top5.avg,\n        'Loss': losses.avg,\n        'Recall': recall_value.avg,\n        'Precision': precision_value.avg,\n        'F1 Score': f1_score_value.avg,\n        'Specificity': specificity_value.avg,\n        'Mean Absolute Error': mean_absolute_error_value.avg,\n        'R2 Score': r2_score_value.avg,\n    }\n\n    return pd.DataFrame.from_dict(results, orient=\"index\", columns=[data_name])","metadata":{"execution":{"iopub.status.busy":"2023-02-04T19:53:04.590680Z","iopub.execute_input":"2023-02-04T19:53:04.591426Z","iopub.status.idle":"2023-02-04T19:53:04.618591Z","shell.execute_reply.started":"2023-02-04T19:53:04.591385Z","shell.execute_reply":"2023-02-04T19:53:04.617161Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Train and Test Module\ndef train(trainloader, model, criterion, optimizer, epoch):\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    top1 = AverageMeter()\n    top5 = AverageMeter()\n    model.train()\n    end = time.time()\n    for i, (inputs, target) in enumerate(trainloader):\n        \n        # measure data loading time\n        data_time.update(time.time() - end)\n        inputs, target = inputs.cuda(), target.cuda()\n        \n        # compute output\n        outputs = model(inputs)\n        loss = criterion(outputs, target)\n        \n        # measure accuracy and record loss\n        prec = accuracy(outputs, target)[0]\n        prec5 = accuracy(outputs, target, (5,))[0]\n        losses.update(loss.item(), inputs.size(0))\n        top1.update(prec.item(), inputs.size(0))\n        top5.update(prec5.item(), inputs.size(0))\n        \n        # compute gradient and do SGD step\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if i % args.print_freq == 0:\n            print('Epoch: [{0}][{1}/{2}]\\t'\n                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n                  'Prec {top1.val:.3f}% ({top1.avg:.3f}%)\\t'\n                  'Prec5 {top5.val:.3f}% ({top5.avg:.3f}%)'.format(\n                epoch, i, len(trainloader), batch_time=batch_time,\n                data_time=data_time, loss=losses, top1=top1, top5=top5))\n    return losses.avg, top1.avg, top5.avg\n\n\ndef test(val_loader, model, criterion):\n    batch_time = AverageMeter()\n    losses = AverageMeter()\n    top1 = AverageMeter()\n    top5 = AverageMeter()\n    \n    # switch to evaluate mode\n    model.eval()\n    end = time.time()\n    with torch.no_grad():\n        for i, (inputs, target) in enumerate(val_loader):\n            inputs, target = inputs.cuda(), target.cuda()\n\n            # compute output\n            output = model(inputs)\n            loss = criterion(output, target)\n\n            # measure accuracy and record loss\n            prec = accuracy(output, target)[0]\n            prec5 = accuracy(output, target, (5,))[0]\n            losses.update(loss.item(), inputs.size(0))\n            top1.update(prec.item(), inputs.size(0))\n            top5.update(prec5.item(), inputs.size(0))\n            # measure elapsed time\n            batch_time.update(time.time() - end)\n            end = time.time()\n\n            if i % args.print_freq == 0:\n                print('Test: [{0}/{1}]\\t'\n                      'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n                      'Prec {top1.val:.3f}% ({top1.avg:.3f}%)\\t'\n                      'Prec5 {top5.val:.3f}% ({top5.avg:.3f}%)'.format(\n                    i, len(val_loader), batch_time=batch_time, loss=losses,\n                    top1=top1, top5=top5))\n\n    print(' * Prec {top1.avg:.3f}% '.format(top1=top1))\n\n    return losses.avg, top1.avg, top5.avg\n","metadata":{"execution":{"iopub.status.busy":"2023-02-04T19:53:04.622982Z","iopub.execute_input":"2023-02-04T19:53:04.623707Z","iopub.status.idle":"2023-02-04T19:53:04.640468Z","shell.execute_reply.started":"2023-02-04T19:53:04.623665Z","shell.execute_reply":"2023-02-04T19:53:04.639527Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# data loading code\ndef laod_data():\n    if args.dataset == 'cifar10':\n        transform_train = [\n            # transforms.RandomCrop(32, padding=4),\n            transforms.Resize(227),\n            transforms.RandomHorizontalFlip(),\n        ]\n\n        transform_train.extend([\n            transforms.ToTensor(),\n            # transforms.Normalize((0.4914, 0.4822, 0.4465),\n            #                      (0.2023, 0.1994, 0.2010)),\n        ])\n        transform_train = transforms.Compose(transform_train)\n\n        transform_test = transforms.Compose([\n            transforms.Resize(227),\n            transforms.ToTensor(),\n            # transforms.Normalize((0.5071, 0.4867, 0.4408),\n            #                      (0.2675, 0.2565, 0.2761)),\n        ])\n\n        train_set = datasets.MNIST(\n            root='~/data',\n            train=True,\n            download=True,\n            transform=transform_train)\n        train_loader = torch.utils.data.DataLoader(\n            train_set,\n            batch_size=args.batch_size_train,\n            shuffle=True,\n            num_workers=8)\n\n        test_set = datasets.MNIST(\n            root='~/data',\n            train=False,\n            download=True,\n            transform=transform_test)\n        test_loader = torch.utils.data.DataLoader(\n            test_set,\n            batch_size=args.batch_size_test,\n            shuffle=False,\n            num_workers=8)\n\n        num_classes = 10\n\n    elif args.dataset == 'fashionMnist':\n        data_transform = transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize((0.1307,), (0.3081,))\n        ])\n\n        trainset = datasets.FashionMNIST(root='./data/', train=True, download=True, transform=data_transform)\n        train_loader = torch.utils.data.DataLoader(trainset, batch_size=args.batch_size_train, shuffle=True,\n                                                   num_workers=2)\n\n        testset = datasets.FashionMNIST(root='./data/', train=False, download=True, transform=data_transform)\n        test_loader = torch.utils.data.DataLoader(testset, batch_size=args.batch_size_test, shuffle=False,\n                                                  num_workers=2)\n        num_classes = 10\n\n    elif args.dataset == 'cifar100':\n        transform_train = [\n            # transforms.RandomCrop(32, padding=4),\n            transforms.Resize(227),\n            transforms.RandomHorizontalFlip(),\n        ]\n        transform_train.extend([\n            transforms.ToTensor(),\n            transforms.Normalize((0.4914, 0.4822, 0.4465),\n                                 (0.2023, 0.1994, 0.2010)),\n        ])\n        transform_train = transforms.Compose(transform_train)\n\n        transform_test = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Resize(227),\n            transforms.Normalize((0.4914, 0.4822, 0.4465),\n                                 (0.2023, 0.1994, 0.2010)),\n        ])\n\n        train_set = datasets.CIFAR100(\n            root='~/data',\n            train=True,\n            download=True,\n            transform=transform_train)\n        train_loader = torch.utils.data.DataLoader(\n            train_set,\n            batch_size=args.batch_size_train,\n            shuffle=True,\n            num_workers=8)\n\n        test_set = datasets.CIFAR100(\n            root='~/data',\n            train=False,\n            download=True,\n            transform=transform_test)\n        test_loader = torch.utils.data.DataLoader(\n            test_set,\n            batch_size=args.batch_size_test,\n            shuffle=False,\n            num_workers=8)\n\n        num_classes = 100\n\n    return num_classes, train_loader, test_loader","metadata":{"execution":{"iopub.status.busy":"2023-02-04T19:53:04.642200Z","iopub.execute_input":"2023-02-04T19:53:04.642582Z","iopub.status.idle":"2023-02-04T19:53:04.659922Z","shell.execute_reply.started":"2023-02-04T19:53:04.642548Z","shell.execute_reply":"2023-02-04T19:53:04.658980Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Arguments and Settings\nparser = argparse.ArgumentParser(description='PyTorch Cifar10 Training')\nparser.add_argument('--epochs', default=30, type=int, metavar='N', help='number of total epochs to run')\nparser.add_argument('--start-epoch', default=0, type=int, metavar='N', help='manual epoch number (useful on restarts)')\nparser.add_argument('-b', '--batch-size-train', default=256, type=int, metavar='N',\n                    help='mini-batch size (default: 128),only used for train')\nparser.add_argument('--batch-size-test', default=256, type=int, metavar='N',\n                    help='mini-batch size (default: 128),only used for train')\nparser.add_argument('--lr', '--learning-rate', default=0.3, type=float, metavar='LR', help='initial learning rate')\nparser.add_argument('--momentum', default=0.9, type=float, metavar='M', help='momentum')\nparser.add_argument('--weight-decay', '--wd', default=5e-4, type=float, metavar='W',\n                    help='weight decay (default: 1e-4)')\nparser.add_argument('--print-freq', '-p', default=100, type=int, metavar='N', help='print frequency (default: 10)')\nparser.add_argument('--resume', default='', type=str, metavar='PATH', help='path to latest checkpoint (default: none)')\nparser.add_argument('-e', '--evaluate', dest='evaluate', action='store_true', help='evaluate model on validation set')\nparser.add_argument('-ct', '--cifar-type', default='10', type=int, metavar='CT',\n                    help='10 for cifar10,100 for cifar100 (default: 10)')\nparser.add_argument('--modelName', default='AlexNet', type=str)\nparser.add_argument('--dataset', default='fashionMnist', choices=['cifar10', 'cifar100', 'fashionMnist'],\n                    help='dataset name')\nargs, unknown = parser.parse_known_args()\nbest_prec = 0","metadata":{"execution":{"iopub.status.busy":"2023-02-04T19:53:04.661215Z","iopub.execute_input":"2023-02-04T19:53:04.661798Z","iopub.status.idle":"2023-02-04T19:53:04.677174Z","shell.execute_reply.started":"2023-02-04T19:53:04.661761Z","shell.execute_reply":"2023-02-04T19:53:04.676273Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Checkpoint Directory\nfdir = f'result/{args.dataset}_{args.modelName}'\nif not os.path.exists(fdir):\n    os.makedirs(fdir)","metadata":{"execution":{"iopub.status.busy":"2023-02-04T19:53:04.678602Z","iopub.execute_input":"2023-02-04T19:53:04.679021Z","iopub.status.idle":"2023-02-04T19:53:04.689755Z","shell.execute_reply.started":"2023-02-04T19:53:04.678985Z","shell.execute_reply":"2023-02-04T19:53:04.688890Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# load data\nnum_classes, train_loader, test_loader = laod_data()\nprint(f'data : {args.dataset} is loaded')","metadata":{"execution":{"iopub.status.busy":"2023-02-04T19:53:04.691329Z","iopub.execute_input":"2023-02-04T19:53:04.691779Z","iopub.status.idle":"2023-02-04T19:53:10.562563Z","shell.execute_reply.started":"2023-02-04T19:53:04.691744Z","shell.execute_reply":"2023-02-04T19:53:10.561252Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/26421880 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a41bafe167cb401896e88c443d64a84d"}},"metadata":{}},{"name":"stdout","text":"Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/29515 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c1b9f6a18244f158b22d844596cd106"}},"metadata":{}},{"name":"stdout","text":"Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4422102 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a69f517bd164921a7fde604ff798e04"}},"metadata":{}},{"name":"stdout","text":"Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5148 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95396c08acfe421da33c94f6393c36c2"}},"metadata":{}},{"name":"stdout","text":"Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n\ndata : fashionMnist is loaded\n","output_type":"stream"}]},{"cell_type":"code","source":"# Set GPU or CPU Computing\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = AlexNet(input_channel=1, num_classes=num_classes)\nprint(model)\nmodel = model.to(device)\n# Set multi GPU Computing\nif device == 'cuda':\n    model = torch.nn.DataParallel(model)\n    cudnn.benchmark = True","metadata":{"execution":{"iopub.status.busy":"2023-02-04T19:53:10.564090Z","iopub.execute_input":"2023-02-04T19:53:10.564652Z","iopub.status.idle":"2023-02-04T19:53:15.003529Z","shell.execute_reply.started":"2023-02-04T19:53:10.564615Z","shell.execute_reply":"2023-02-04T19:53:15.002532Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"AlexNet(\n  (features): Sequential(\n    (0): Conv2d(1, 96, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n    (1): ReLU()\n    (2): LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=2)\n    (3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (4): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (5): ReLU()\n    (6): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (7): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (8): ReLU()\n    (9): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (10): ReLU()\n    (11): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (12): ReLU()\n    (13): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (classifier): Sequential(\n    (0): Dropout(p=0.5, inplace=False)\n    (1): Linear(in_features=9216, out_features=4096, bias=True)\n    (2): ReLU()\n    (3): Dropout(p=0.5, inplace=False)\n    (4): Linear(in_features=4096, out_features=4096, bias=True)\n    (5): ReLU()\n    (6): Linear(in_features=4096, out_features=10, bias=True)\n    (7): Softmax(dim=None)\n  )\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Model Settings\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer=optimizer, step_size=5)","metadata":{"execution":{"iopub.status.busy":"2023-02-04T19:53:15.006902Z","iopub.execute_input":"2023-02-04T19:53:15.007263Z","iopub.status.idle":"2023-02-04T19:53:15.012150Z","shell.execute_reply.started":"2023-02-04T19:53:15.007227Z","shell.execute_reply":"2023-02-04T19:53:15.011240Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"train_loss_epoch = []\ntest_loss_epoch = []\nacc_train_1 = []\nacc_test_1 = []\nacc_train_5 = []\nacc_test_5 = []\nlearning_rates = []\nfor epoch in range(args.start_epoch, args.epochs):\n    model.train(True)\n    train_loss, top1_train, top5_train = train(train_loader, model, criterion, optimizer, epoch)\n    train_loss_epoch.append(train_loss)\n\n    # evaluate on test set\n    test_loss, top1_test, top5_test = test(test_loader, model, criterion)\n    test_loss_epoch.append(test_loss)\n    acc_train_1.append(top1_train)\n    acc_test_1.append(top1_test)\n    acc_train_5.append(top5_train)\n    acc_test_5.append(top5_test)\n    scheduler.step()\n    print(f\"Train |Loss : {train_loss_epoch[epoch]}|average accuracy Top-1 : {acc_train_1[epoch]}|\"\n          f\"average accuracy Top-5 : {acc_train_5[epoch]}|\")\n    print(f\"Train |Loss : {test_loss_epoch[epoch]} | average accuracy Top-1 : {acc_test_1[epoch]}|\"\n          f\"average accuracy Top-5 : {acc_test_5[epoch]}|\")\n\n    learning_rate = optimizer.param_groups[0]['lr']\n    learning_rates.append(learning_rate)\n    is_best = top1_test > best_prec\n    if is_best:\n        best_val_model = deepcopy(model.state_dict())\n        best_lr = learning_rate\n    best_prec = max(top1_test, best_prec)\n    save_checkpoint({\n        'epoch': epoch + 1,\n        'state_dict': model.state_dict(),\n        'best_prec': best_prec,\n        'optimizer': optimizer.state_dict(),\n    }, is_best, fdir)\n\n    print(f\"*********************** Epoch : {epoch}  ********************\")\nprint('End of Training')","metadata":{"execution":{"iopub.status.busy":"2023-02-04T19:53:15.014685Z","iopub.execute_input":"2023-02-04T19:53:15.015284Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/container.py:141: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  input = module(input)\n","output_type":"stream"},{"name":"stdout","text":"Epoch: [0][0/469]\tTime 8.540 (8.540)\tData 0.311 (0.311)\tLoss 2.3025 (2.3025)\tPrec 8.594% (8.594%)\tPrec5 53.906% (53.906%)\nEpoch: [0][100/469]\tTime 0.155 (0.238)\tData 0.010 (0.018)\tLoss 2.3024 (2.3024)\tPrec 10.156% (11.371%)\tPrec5 49.219% (51.663%)\nEpoch: [0][200/469]\tTime 0.163 (0.198)\tData 0.010 (0.019)\tLoss 2.3671 (2.3036)\tPrec 9.375% (13.176%)\tPrec5 69.531% (53.836%)\nEpoch: [0][300/469]\tTime 0.159 (0.185)\tData 0.015 (0.018)\tLoss 2.0318 (2.2573)\tPrec 42.188% (18.226%)\tPrec5 93.750% (63.468%)\nEpoch: [0][400/469]\tTime 0.165 (0.179)\tData 0.011 (0.017)\tLoss 1.8355 (2.1805)\tPrec 62.500% (26.375%)\tPrec5 90.625% (70.630%)\nTest: [0/79]\tTime 0.397 (0.397)\tLoss 1.7992 (1.7992)\tPrec 66.406% (66.406%)\tPrec5 91.406% (91.406%)\n * Prec 62.920% \nTrain |Loss : 2.1386890657424926|average accuracy Top-1 : 30.799999997965493|average accuracy Top-5 : 73.46833333333333|\nTrain |Loss : 1.828092121696472 | average accuracy Top-1 : 62.92|average accuracy Top-5 : 90.62|\n*********************** Epoch : 0  ********************\nEpoch: [1][0/469]\tTime 0.422 (0.422)\tData 0.333 (0.333)\tLoss 1.7976 (1.7976)\tPrec 65.625% (65.625%)\tPrec5 93.750% (93.750%)\nEpoch: [1][100/469]\tTime 0.167 (0.172)\tData 0.013 (0.017)\tLoss 1.7641 (1.7991)\tPrec 69.531% (66.228%)\tPrec5 87.500% (90.873%)\nEpoch: [1][200/469]\tTime 0.170 (0.171)\tData 0.014 (0.014)\tLoss 1.7433 (1.7845)\tPrec 71.094% (67.666%)\tPrec5 91.406% (91.647%)\nEpoch: [1][300/469]\tTime 0.167 (0.170)\tData 0.010 (0.013)\tLoss 1.6872 (1.7787)\tPrec 78.125% (68.223%)\tPrec5 93.750% (92.343%)\nEpoch: [1][400/469]\tTime 0.166 (0.169)\tData 0.009 (0.014)\tLoss 1.7909 (1.7701)\tPrec 67.188% (69.107%)\tPrec5 94.531% (93.216%)\nTest: [0/79]\tTime 0.402 (0.402)\tLoss 1.7085 (1.7085)\tPrec 75.781% (75.781%)\tPrec5 96.875% (96.875%)\n * Prec 71.600% \nTrain |Loss : 1.7660219280878702|average accuracy Top-1 : 69.52333332926432|average accuracy Top-5 : 93.655|\nTrain |Loss : 1.7448689456939697 | average accuracy Top-1 : 71.6|average accuracy Top-5 : 95.39|\n*********************** Epoch : 1  ********************\nEpoch: [2][0/469]\tTime 0.389 (0.389)\tData 0.322 (0.322)\tLoss 1.7425 (1.7425)\tPrec 71.094% (71.094%)\tPrec5 93.750% (93.750%)\nEpoch: [2][100/469]\tTime 0.166 (0.172)\tData 0.006 (0.022)\tLoss 1.7815 (1.7392)\tPrec 69.531% (72.123%)\tPrec5 94.531% (96.248%)\nEpoch: [2][200/469]\tTime 0.158 (0.170)\tData 0.022 (0.016)\tLoss 1.7460 (1.7336)\tPrec 71.875% (72.718%)\tPrec5 97.656% (96.514%)\nEpoch: [2][300/469]\tTime 0.163 (0.170)\tData 0.018 (0.015)\tLoss 1.6836 (1.7263)\tPrec 77.344% (73.482%)\tPrec5 96.875% (96.387%)\nEpoch: [2][400/469]\tTime 0.172 (0.169)\tData 0.038 (0.014)\tLoss 1.6808 (1.7208)\tPrec 78.125% (74.051%)\tPrec5 98.438% (96.158%)\nTest: [0/79]\tTime 0.399 (0.399)\tLoss 1.6649 (1.6649)\tPrec 79.688% (79.688%)\tPrec5 97.656% (97.656%)\n * Prec 77.420% \nTrain |Loss : 1.716530265045166|average accuracy Top-1 : 74.47333332926432|average accuracy Top-5 : 96.20666666666666|\nTrain |Loss : 1.6847865724563598 | average accuracy Top-1 : 77.42|average accuracy Top-5 : 96.22|\n*********************** Epoch : 2  ********************\nEpoch: [3][0/469]\tTime 0.452 (0.452)\tData 0.356 (0.356)\tLoss 1.6800 (1.6800)\tPrec 78.125% (78.125%)\tPrec5 96.094% (96.094%)\nEpoch: [3][100/469]\tTime 0.167 (0.171)\tData 0.006 (0.020)\tLoss 1.6746 (1.6870)\tPrec 78.125% (77.444%)\tPrec5 96.875% (96.550%)\nEpoch: [3][200/469]\tTime 0.165 (0.170)\tData 0.017 (0.015)\tLoss 1.6689 (1.6896)\tPrec 78.906% (77.118%)\tPrec5 97.656% (96.226%)\nEpoch: [3][300/469]\tTime 0.167 (0.169)\tData 0.010 (0.014)\tLoss 1.6291 (1.6834)\tPrec 83.594% (77.741%)\tPrec5 97.656% (96.470%)\nEpoch: [3][400/469]\tTime 0.169 (0.169)\tData 0.006 (0.013)\tLoss 1.6865 (1.6827)\tPrec 77.344% (77.821%)\tPrec5 97.656% (96.698%)\nTest: [0/79]\tTime 0.402 (0.402)\tLoss 1.6415 (1.6415)\tPrec 82.031% (82.031%)\tPrec5 100.000% (100.000%)\n * Prec 78.720% \nTrain |Loss : 1.6815673164367675|average accuracy Top-1 : 77.92833332926432|average accuracy Top-5 : 96.76499999186198|\nTrain |Loss : 1.6725744800567628 | average accuracy Top-1 : 78.72|average accuracy Top-5 : 98.64|\n*********************** Epoch : 3  ********************\nEpoch: [4][0/469]\tTime 0.442 (0.442)\tData 0.343 (0.343)\tLoss 1.6689 (1.6689)\tPrec 78.906% (78.906%)\tPrec5 96.094% (96.094%)\nEpoch: [4][100/469]\tTime 0.169 (0.170)\tData 0.016 (0.017)\tLoss 1.6174 (1.6732)\tPrec 83.594% (78.752%)\tPrec5 96.875% (97.146%)\nEpoch: [4][200/469]\tTime 0.169 (0.169)\tData 0.010 (0.014)\tLoss 1.6781 (1.6748)\tPrec 78.906% (78.599%)\tPrec5 97.656% (97.477%)\nEpoch: [4][300/469]\tTime 0.166 (0.169)\tData 0.010 (0.014)\tLoss 1.6979 (1.6716)\tPrec 76.562% (78.961%)\tPrec5 98.438% (97.654%)\nEpoch: [4][400/469]\tTime 0.163 (0.168)\tData 0.010 (0.013)\tLoss 1.6448 (1.6700)\tPrec 82.812% (79.117%)\tPrec5 98.438% (97.847%)\nTest: [0/79]\tTime 0.534 (0.534)\tLoss 1.6475 (1.6475)\tPrec 81.250% (81.250%)\tPrec5 100.000% (100.000%)\n * Prec 79.190% \nTrain |Loss : 1.6689772687911988|average accuracy Top-1 : 79.22499999593099|average accuracy Top-5 : 97.90333332519532|\nTrain |Loss : 1.6694113883972168 | average accuracy Top-1 : 79.19|average accuracy Top-5 : 98.65|\n*********************** Epoch : 4  ********************\nEpoch: [5][0/469]\tTime 0.421 (0.421)\tData 0.355 (0.355)\tLoss 1.6779 (1.6779)\tPrec 78.906% (78.906%)\tPrec5 99.219% (99.219%)\nEpoch: [5][100/469]\tTime 0.172 (0.168)\tData 0.006 (0.015)\tLoss 1.7307 (1.6643)\tPrec 72.656% (79.711%)\tPrec5 97.656% (98.391%)\nEpoch: [5][200/469]\tTime 0.166 (0.169)\tData 0.006 (0.014)\tLoss 1.6701 (1.6594)\tPrec 78.906% (80.232%)\tPrec5 99.219% (98.523%)\nEpoch: [5][300/469]\tTime 0.169 (0.168)\tData 0.006 (0.012)\tLoss 1.6616 (1.6576)\tPrec 79.688% (80.386%)\tPrec5 96.094% (98.534%)\nEpoch: [5][400/469]\tTime 0.169 (0.168)\tData 0.006 (0.013)\tLoss 1.6800 (1.6555)\tPrec 78.125% (80.584%)\tPrec5 98.438% (98.547%)\nTest: [0/79]\tTime 0.389 (0.389)\tLoss 1.6351 (1.6351)\tPrec 82.031% (82.031%)\tPrec5 99.219% (99.219%)\n * Prec 80.510% \nTrain |Loss : 1.6534273734410603|average accuracy Top-1 : 80.785|average accuracy Top-5 : 98.58333333333333|\nTrain |Loss : 1.6555543025970458 | average accuracy Top-1 : 80.51|average accuracy Top-5 : 98.95|\n*********************** Epoch : 5  ********************\nEpoch: [6][0/469]\tTime 0.436 (0.436)\tData 0.361 (0.361)\tLoss 1.7119 (1.7119)\tPrec 75.000% (75.000%)\tPrec5 98.438% (98.438%)\nEpoch: [6][100/469]\tTime 0.176 (0.171)\tData 0.022 (0.016)\tLoss 1.6502 (1.6450)\tPrec 79.688% (81.575%)\tPrec5 99.219% (98.724%)\nEpoch: [6][200/469]\tTime 0.166 (0.170)\tData 0.010 (0.014)\tLoss 1.6428 (1.6459)\tPrec 82.031% (81.507%)\tPrec5 99.219% (98.799%)\nEpoch: [6][300/469]\tTime 0.170 (0.169)\tData 0.019 (0.013)\tLoss 1.6646 (1.6480)\tPrec 79.688% (81.310%)\tPrec5 98.438% (98.684%)\nEpoch: [6][400/469]\tTime 0.163 (0.169)\tData 0.009 (0.014)\tLoss 1.6507 (1.6485)\tPrec 80.469% (81.269%)\tPrec5 99.219% (98.695%)\nTest: [0/79]\tTime 0.388 (0.388)\tLoss 1.6361 (1.6361)\tPrec 82.812% (82.812%)\tPrec5 99.219% (99.219%)\n * Prec 80.780% \nTrain |Loss : 1.6481392151514689|average accuracy Top-1 : 81.295|average accuracy Top-5 : 98.6833333251953|\nTrain |Loss : 1.6526694408416749 | average accuracy Top-1 : 80.78|average accuracy Top-5 : 98.85|\n*********************** Epoch : 6  ********************\nEpoch: [7][0/469]\tTime 0.450 (0.450)\tData 0.354 (0.354)\tLoss 1.6470 (1.6470)\tPrec 80.469% (80.469%)\tPrec5 99.219% (99.219%)\nEpoch: [7][100/469]\tTime 0.170 (0.171)\tData 0.007 (0.017)\tLoss 1.5943 (1.6460)\tPrec 85.938% (81.498%)\tPrec5 99.219% (98.507%)\nEpoch: [7][200/469]\tTime 0.164 (0.170)\tData 0.006 (0.015)\tLoss 1.6775 (1.6446)\tPrec 78.125% (81.596%)\tPrec5 97.656% (98.690%)\nEpoch: [7][300/469]\tTime 0.167 (0.169)\tData 0.006 (0.014)\tLoss 1.6188 (1.6448)\tPrec 84.375% (81.595%)\tPrec5 99.219% (98.715%)\nEpoch: [7][400/469]\tTime 0.162 (0.169)\tData 0.010 (0.015)\tLoss 1.6268 (1.6458)\tPrec 82.812% (81.507%)\tPrec5 99.219% (98.757%)\nTest: [0/79]\tTime 0.396 (0.396)\tLoss 1.6352 (1.6352)\tPrec 82.812% (82.812%)\tPrec5 99.219% (99.219%)\n * Prec 81.090% \nTrain |Loss : 1.6465114336649578|average accuracy Top-1 : 81.44333332519531|average accuracy Top-5 : 98.76999999593099|\nTrain |Loss : 1.6498695936203003 | average accuracy Top-1 : 81.09|average accuracy Top-5 : 99.0|\n*********************** Epoch : 7  ********************\nEpoch: [8][0/469]\tTime 0.433 (0.433)\tData 0.363 (0.363)\tLoss 1.6341 (1.6341)\tPrec 82.812% (82.812%)\tPrec5 99.219% (99.219%)\nEpoch: [8][100/469]\tTime 0.167 (0.172)\tData 0.010 (0.020)\tLoss 1.5968 (1.6469)\tPrec 86.719% (81.536%)\tPrec5 96.875% (98.894%)\nEpoch: [8][200/469]\tTime 0.169 (0.170)\tData 0.006 (0.015)\tLoss 1.6547 (1.6447)\tPrec 80.469% (81.693%)\tPrec5 95.312% (98.923%)\nEpoch: [8][300/469]\tTime 0.164 (0.170)\tData 0.008 (0.016)\tLoss 1.6574 (1.6431)\tPrec 80.469% (81.873%)\tPrec5 97.656% (98.905%)\nEpoch: [8][400/469]\tTime 0.263 (0.169)\tData 0.193 (0.015)\tLoss 1.6104 (1.6432)\tPrec 84.375% (81.854%)\tPrec5 100.000% (98.905%)\nTest: [0/79]\tTime 0.398 (0.398)\tLoss 1.6375 (1.6375)\tPrec 82.812% (82.812%)\tPrec5 99.219% (99.219%)\n * Prec 81.220% \nTrain |Loss : 1.644591368929545|average accuracy Top-1 : 81.72166666666666|average accuracy Top-5 : 98.915|\nTrain |Loss : 1.6490990739822389 | average accuracy Top-1 : 81.22|average accuracy Top-5 : 99.16|\n*********************** Epoch : 8  ********************\nEpoch: [9][0/469]\tTime 0.487 (0.487)\tData 0.387 (0.387)\tLoss 1.6397 (1.6397)\tPrec 81.250% (81.250%)\tPrec5 99.219% (99.219%)\nEpoch: [9][100/469]\tTime 0.168 (0.172)\tData 0.012 (0.020)\tLoss 1.5931 (1.6434)\tPrec 86.719% (81.784%)\tPrec5 100.000% (99.095%)\nEpoch: [9][200/469]\tTime 0.167 (0.171)\tData 0.016 (0.015)\tLoss 1.6247 (1.6443)\tPrec 83.594% (81.755%)\tPrec5 98.438% (98.989%)\nEpoch: [9][300/469]\tTime 0.165 (0.170)\tData 0.011 (0.015)\tLoss 1.6215 (1.6440)\tPrec 83.594% (81.730%)\tPrec5 99.219% (99.011%)\nEpoch: [9][400/469]\tTime 0.162 (0.169)\tData 0.017 (0.014)\tLoss 1.6408 (1.6439)\tPrec 81.250% (81.747%)\tPrec5 100.000% (99.028%)\nTest: [0/79]\tTime 0.423 (0.423)\tLoss 1.6392 (1.6392)\tPrec 82.812% (82.812%)\tPrec5 98.438% (98.438%)\n * Prec 81.370% \nTrain |Loss : 1.6432337683995564|average accuracy Top-1 : 81.81666666259765|average accuracy Top-5 : 99.05499999186198|\nTrain |Loss : 1.6474485164642334 | average accuracy Top-1 : 81.37|average accuracy Top-5 : 99.26|\n*********************** Epoch : 9  ********************\nEpoch: [10][0/469]\tTime 0.412 (0.412)\tData 0.346 (0.346)\tLoss 1.6535 (1.6535)\tPrec 81.250% (81.250%)\tPrec5 96.875% (96.875%)\nEpoch: [10][100/469]\tTime 0.165 (0.171)\tData 0.006 (0.017)\tLoss 1.5899 (1.6436)\tPrec 87.500% (81.753%)\tPrec5 100.000% (99.149%)\nEpoch: [10][200/469]\tTime 0.167 (0.170)\tData 0.010 (0.014)\tLoss 1.6771 (1.6406)\tPrec 77.344% (82.023%)\tPrec5 98.438% (99.137%)\nEpoch: [10][300/469]\tTime 0.168 (0.169)\tData 0.012 (0.014)\tLoss 1.6491 (1.6418)\tPrec 81.250% (81.917%)\tPrec5 99.219% (99.175%)\nEpoch: [10][400/469]\tTime 0.163 (0.168)\tData 0.012 (0.014)\tLoss 1.6440 (1.6415)\tPrec 82.031% (81.947%)\tPrec5 100.000% (99.131%)\nTest: [0/79]\tTime 0.407 (0.407)\tLoss 1.6365 (1.6365)\tPrec 82.031% (82.031%)\tPrec5 99.219% (99.219%)\n * Prec 81.540% \nTrain |Loss : 1.6413864671707152|average accuracy Top-1 : 81.965|average accuracy Top-5 : 99.104999995931|\nTrain |Loss : 1.6460460559844972 | average accuracy Top-1 : 81.54|average accuracy Top-5 : 99.24|\n*********************** Epoch : 10  ********************\nEpoch: [11][0/469]\tTime 0.445 (0.445)\tData 0.349 (0.349)\tLoss 1.6463 (1.6463)\tPrec 81.250% (81.250%)\tPrec5 100.000% (100.000%)\nEpoch: [11][100/469]\tTime 0.170 (0.170)\tData 0.010 (0.015)\tLoss 1.6858 (1.6355)\tPrec 77.344% (82.635%)\tPrec5 98.438% (99.226%)\nEpoch: [11][200/469]\tTime 0.169 (0.170)\tData 0.007 (0.015)\tLoss 1.6316 (1.6383)\tPrec 82.812% (82.323%)\tPrec5 99.219% (99.149%)\nEpoch: [11][300/469]\tTime 0.178 (0.169)\tData 0.038 (0.014)\tLoss 1.6767 (1.6393)\tPrec 78.125% (82.221%)\tPrec5 96.875% (99.136%)\nEpoch: [11][400/469]\tTime 0.162 (0.169)\tData 0.012 (0.015)\tLoss 1.6792 (1.6407)\tPrec 78.125% (82.070%)\tPrec5 99.219% (99.121%)\nTest: [0/79]\tTime 0.400 (0.400)\tLoss 1.6363 (1.6363)\tPrec 82.031% (82.031%)\tPrec5 99.219% (99.219%)\n * Prec 81.560% \nTrain |Loss : 1.6406219029108684|average accuracy Top-1 : 82.059999995931|average accuracy Top-5 : 99.14999999593098|\nTrain |Loss : 1.6458799823760986 | average accuracy Top-1 : 81.56|average accuracy Top-5 : 99.28|\n*********************** Epoch : 11  ********************\nEpoch: [12][0/469]\tTime 0.438 (0.438)\tData 0.359 (0.359)\tLoss 1.6501 (1.6501)\tPrec 81.250% (81.250%)\tPrec5 99.219% (99.219%)\nEpoch: [12][100/469]\tTime 0.168 (0.170)\tData 0.011 (0.016)\tLoss 1.6444 (1.6457)\tPrec 82.031% (81.598%)\tPrec5 99.219% (99.080%)\nEpoch: [12][200/469]\tTime 0.162 (0.170)\tData 0.016 (0.014)\tLoss 1.6314 (1.6452)\tPrec 82.812% (81.627%)\tPrec5 100.000% (99.110%)\nEpoch: [12][300/469]\tTime 0.166 (0.169)\tData 0.025 (0.014)\tLoss 1.6159 (1.6425)\tPrec 85.156% (81.917%)\tPrec5 99.219% (99.141%)\nEpoch: [12][400/469]\tTime 0.164 (0.168)\tData 0.011 (0.014)\tLoss 1.6771 (1.6415)\tPrec 78.125% (82.027%)\tPrec5 100.000% (99.164%)\nTest: [0/79]\tTime 0.421 (0.421)\tLoss 1.6350 (1.6350)\tPrec 82.812% (82.812%)\tPrec5 99.219% (99.219%)\n * Prec 81.520% \nTrain |Loss : 1.6405900765101116|average accuracy Top-1 : 82.1|average accuracy Top-5 : 99.15999999186198|\nTrain |Loss : 1.645891784286499 | average accuracy Top-1 : 81.52|average accuracy Top-5 : 99.25|\n*********************** Epoch : 12  ********************\nEpoch: [13][0/469]\tTime 0.427 (0.427)\tData 0.359 (0.359)\tLoss 1.6171 (1.6171)\tPrec 84.375% (84.375%)\tPrec5 98.438% (98.438%)\nEpoch: [13][100/469]\tTime 0.163 (0.170)\tData 0.008 (0.016)\tLoss 1.6750 (1.6406)\tPrec 78.906% (82.070%)\tPrec5 97.656% (99.172%)\nEpoch: [13][200/469]\tTime 0.170 (0.170)\tData 0.007 (0.015)\tLoss 1.6299 (1.6409)\tPrec 82.812% (82.082%)\tPrec5 98.438% (99.172%)\nEpoch: [13][300/469]\tTime 0.164 (0.169)\tData 0.008 (0.014)\tLoss 1.6396 (1.6419)\tPrec 82.031% (81.998%)\tPrec5 98.438% (99.149%)\nEpoch: [13][400/469]\tTime 0.167 (0.168)\tData 0.006 (0.014)\tLoss 1.6378 (1.6409)\tPrec 82.812% (82.076%)\tPrec5 99.219% (99.164%)\nTest: [0/79]\tTime 0.483 (0.483)\tLoss 1.6335 (1.6335)\tPrec 82.812% (82.812%)\tPrec5 99.219% (99.219%)\n * Prec 81.620% \nTrain |Loss : 1.6404894337336222|average accuracy Top-1 : 82.10333332926432|average accuracy Top-5 : 99.16166665852865|\nTrain |Loss : 1.6455642456054687 | average accuracy Top-1 : 81.62|average accuracy Top-5 : 99.25|\n*********************** Epoch : 13  ********************\nEpoch: [14][0/469]\tTime 0.462 (0.462)\tData 0.385 (0.385)\tLoss 1.6022 (1.6022)\tPrec 85.938% (85.938%)\tPrec5 99.219% (99.219%)\nEpoch: [14][100/469]\tTime 0.169 (0.173)\tData 0.006 (0.021)\tLoss 1.6735 (1.6393)\tPrec 77.344% (82.147%)\tPrec5 100.000% (99.219%)\nEpoch: [14][200/469]\tTime 0.193 (0.171)\tData 0.040 (0.016)\tLoss 1.6200 (1.6412)\tPrec 84.375% (81.957%)\tPrec5 98.438% (99.195%)\nEpoch: [14][300/469]\tTime 0.166 (0.170)\tData 0.020 (0.016)\tLoss 1.6294 (1.6402)\tPrec 83.594% (82.049%)\tPrec5 100.000% (99.242%)\nEpoch: [14][400/469]\tTime 0.175 (0.170)\tData 0.025 (0.016)\tLoss 1.6736 (1.6403)\tPrec 78.125% (82.057%)\tPrec5 98.438% (99.205%)\nTest: [0/79]\tTime 0.407 (0.407)\tLoss 1.6372 (1.6372)\tPrec 82.031% (82.031%)\tPrec5 99.219% (99.219%)\n * Prec 81.570% \nTrain |Loss : 1.6402411520004272|average accuracy Top-1 : 82.06666666666666|average accuracy Top-5 : 99.20666665852865|\nTrain |Loss : 1.6454166786193847 | average accuracy Top-1 : 81.57|average accuracy Top-5 : 99.25|\n*********************** Epoch : 14  ********************\nEpoch: [15][0/469]\tTime 0.417 (0.417)\tData 0.358 (0.358)\tLoss 1.6669 (1.6669)\tPrec 78.906% (78.906%)\tPrec5 99.219% (99.219%)\nEpoch: [15][100/469]\tTime 0.166 (0.171)\tData 0.010 (0.019)\tLoss 1.6252 (1.6396)\tPrec 83.594% (82.062%)\tPrec5 100.000% (99.234%)\nEpoch: [15][200/469]\tTime 0.166 (0.169)\tData 0.009 (0.015)\tLoss 1.6311 (1.6417)\tPrec 83.594% (81.946%)\tPrec5 99.219% (99.234%)\nEpoch: [15][300/469]\tTime 0.161 (0.169)\tData 0.007 (0.014)\tLoss 1.5769 (1.6419)\tPrec 89.062% (81.961%)\tPrec5 100.000% (99.229%)\nEpoch: [15][400/469]\tTime 0.167 (0.168)\tData 0.010 (0.013)\tLoss 1.6602 (1.6411)\tPrec 79.688% (82.027%)\tPrec5 100.000% (99.213%)\nTest: [0/79]\tTime 0.423 (0.423)\tLoss 1.6363 (1.6363)\tPrec 82.031% (82.031%)\tPrec5 99.219% (99.219%)\n * Prec 81.560% \nTrain |Loss : 1.6403745168685913|average accuracy Top-1 : 82.08499999186198|average accuracy Top-5 : 99.21166666666667|\nTrain |Loss : 1.6453057201385497 | average accuracy Top-1 : 81.56|average accuracy Top-5 : 99.26|\n*********************** Epoch : 15  ********************\nEpoch: [16][0/469]\tTime 0.421 (0.421)\tData 0.350 (0.350)\tLoss 1.6908 (1.6908)\tPrec 76.562% (76.562%)\tPrec5 100.000% (100.000%)\nEpoch: [16][100/469]\tTime 0.167 (0.171)\tData 0.009 (0.017)\tLoss 1.6058 (1.6386)\tPrec 85.156% (82.271%)\tPrec5 98.438% (99.350%)\nEpoch: [16][200/469]\tTime 0.168 (0.170)\tData 0.008 (0.015)\tLoss 1.6176 (1.6398)\tPrec 84.375% (82.167%)\tPrec5 100.000% (99.246%)\nEpoch: [16][300/469]\tTime 0.166 (0.169)\tData 0.010 (0.014)\tLoss 1.6444 (1.6393)\tPrec 81.250% (82.223%)\tPrec5 99.219% (99.219%)\nEpoch: [16][400/469]\tTime 0.165 (0.169)\tData 0.010 (0.014)\tLoss 1.6121 (1.6399)\tPrec 85.156% (82.154%)\tPrec5 100.000% (99.188%)\nTest: [0/79]\tTime 0.433 (0.433)\tLoss 1.6356 (1.6356)\tPrec 82.812% (82.812%)\tPrec5 99.219% (99.219%)\n * Prec 81.620% \nTrain |Loss : 1.6397744422276814|average accuracy Top-1 : 82.17833332519531|average accuracy Top-5 : 99.19|\nTrain |Loss : 1.6452815519332886 | average accuracy Top-1 : 81.62|average accuracy Top-5 : 99.26|\n*********************** Epoch : 16  ********************\nEpoch: [17][0/469]\tTime 0.473 (0.473)\tData 0.407 (0.407)\tLoss 1.6627 (1.6627)\tPrec 79.688% (79.688%)\tPrec5 99.219% (99.219%)\nEpoch: [17][100/469]\tTime 0.165 (0.170)\tData 0.016 (0.017)\tLoss 1.6119 (1.6385)\tPrec 84.375% (82.310%)\tPrec5 100.000% (99.188%)\nEpoch: [17][200/469]\tTime 0.166 (0.170)\tData 0.010 (0.015)\tLoss 1.6102 (1.6387)\tPrec 84.375% (82.307%)\tPrec5 99.219% (99.242%)\nEpoch: [17][300/469]\tTime 0.175 (0.169)\tData 0.023 (0.015)\tLoss 1.6274 (1.6389)\tPrec 82.812% (82.280%)\tPrec5 97.656% (99.229%)\nEpoch: [17][400/469]\tTime 0.163 (0.169)\tData 0.010 (0.016)\tLoss 1.6473 (1.6398)\tPrec 81.250% (82.172%)\tPrec5 100.000% (99.199%)\nTest: [0/79]\tTime 0.411 (0.411)\tLoss 1.6348 (1.6348)\tPrec 82.812% (82.812%)\tPrec5 99.219% (99.219%)\n * Prec 81.640% \nTrain |Loss : 1.6397205152511596|average accuracy Top-1 : 82.17666666666666|average accuracy Top-5 : 99.21|\nTrain |Loss : 1.6452128562927246 | average accuracy Top-1 : 81.64|average accuracy Top-5 : 99.26|\n*********************** Epoch : 17  ********************\nEpoch: [18][0/469]\tTime 0.424 (0.424)\tData 0.362 (0.362)\tLoss 1.6113 (1.6113)\tPrec 85.156% (85.156%)\tPrec5 99.219% (99.219%)\nEpoch: [18][100/469]\tTime 0.173 (0.170)\tData 0.017 (0.019)\tLoss 1.5899 (1.6391)\tPrec 87.500% (82.201%)\tPrec5 100.000% (99.211%)\nEpoch: [18][200/469]\tTime 0.168 (0.170)\tData 0.011 (0.016)\tLoss 1.6895 (1.6400)\tPrec 77.344% (82.144%)\tPrec5 98.438% (99.180%)\nEpoch: [18][300/469]\tTime 0.166 (0.169)\tData 0.011 (0.015)\tLoss 1.6374 (1.6404)\tPrec 82.812% (82.099%)\tPrec5 98.438% (99.167%)\nEpoch: [18][400/469]\tTime 0.163 (0.169)\tData 0.015 (0.015)\tLoss 1.6112 (1.6397)\tPrec 85.156% (82.173%)\tPrec5 99.219% (99.180%)\nTest: [0/79]\tTime 0.423 (0.423)\tLoss 1.6350 (1.6350)\tPrec 82.812% (82.812%)\tPrec5 99.219% (99.219%)\n * Prec 81.630% \nTrain |Loss : 1.639877845064799|average accuracy Top-1 : 82.15833332926432|average accuracy Top-5 : 99.19333332519531|\nTrain |Loss : 1.64521620388031 | average accuracy Top-1 : 81.63|average accuracy Top-5 : 99.26|\n*********************** Epoch : 18  ********************\nEpoch: [19][0/469]\tTime 0.423 (0.423)\tData 0.353 (0.353)\tLoss 1.6230 (1.6230)\tPrec 83.594% (83.594%)\tPrec5 100.000% (100.000%)\nEpoch: [19][100/469]\tTime 0.170 (0.171)\tData 0.010 (0.017)\tLoss 1.6070 (1.6369)\tPrec 85.156% (82.565%)\tPrec5 99.219% (99.250%)\nEpoch: [19][200/469]\tTime 0.164 (0.171)\tData 0.030 (0.016)\tLoss 1.6234 (1.6382)\tPrec 84.375% (82.369%)\tPrec5 98.438% (99.234%)\nEpoch: [19][300/469]\tTime 0.166 (0.170)\tData 0.011 (0.016)\tLoss 1.6171 (1.6387)\tPrec 85.156% (82.288%)\tPrec5 100.000% (99.214%)\nEpoch: [19][400/469]\tTime 0.183 (0.169)\tData 0.016 (0.016)\tLoss 1.6585 (1.6403)\tPrec 79.688% (82.078%)\tPrec5 98.438% (99.203%)\nTest: [0/79]\tTime 0.426 (0.426)\tLoss 1.6352 (1.6352)\tPrec 82.812% (82.812%)\tPrec5 99.219% (99.219%)\n * Prec 81.620% \nTrain |Loss : 1.639898289235433|average accuracy Top-1 : 82.12999999593099|average accuracy Top-5 : 99.21|\nTrain |Loss : 1.645216478729248 | average accuracy Top-1 : 81.62|average accuracy Top-5 : 99.26|\n*********************** Epoch : 19  ********************\nEpoch: [20][0/469]\tTime 0.553 (0.553)\tData 0.474 (0.474)\tLoss 1.5642 (1.5642)\tPrec 89.844% (89.844%)\tPrec5 98.438% (98.438%)\nEpoch: [20][100/469]\tTime 0.165 (0.174)\tData 0.016 (0.025)\tLoss 1.6092 (1.6402)\tPrec 85.156% (82.147%)\tPrec5 100.000% (99.080%)\nEpoch: [20][200/469]\tTime 0.200 (0.172)\tData 0.054 (0.019)\tLoss 1.6014 (1.6409)\tPrec 86.719% (82.090%)\tPrec5 99.219% (99.199%)\nEpoch: [20][300/469]\tTime 0.168 (0.171)\tData 0.011 (0.018)\tLoss 1.6644 (1.6402)\tPrec 80.469% (82.132%)\tPrec5 99.219% (99.219%)\nEpoch: [20][400/469]\tTime 0.178 (0.170)\tData 0.017 (0.018)\tLoss 1.6271 (1.6395)\tPrec 84.375% (82.212%)\tPrec5 100.000% (99.223%)\nTest: [0/79]\tTime 0.413 (0.413)\tLoss 1.6352 (1.6352)\tPrec 82.812% (82.812%)\tPrec5 99.219% (99.219%)\n * Prec 81.620% \nTrain |Loss : 1.6396569007873536|average accuracy Top-1 : 82.20166665852865|average accuracy Top-5 : 99.23166665852865|\nTrain |Loss : 1.6452116147994995 | average accuracy Top-1 : 81.62|average accuracy Top-5 : 99.26|\n*********************** Epoch : 20  ********************\nEpoch: [21][0/469]\tTime 0.427 (0.427)\tData 0.365 (0.365)\tLoss 1.6164 (1.6164)\tPrec 84.375% (84.375%)\tPrec5 98.438% (98.438%)\nEpoch: [21][100/469]\tTime 0.168 (0.171)\tData 0.010 (0.019)\tLoss 1.6091 (1.6403)\tPrec 85.156% (82.008%)\tPrec5 99.219% (99.288%)\nEpoch: [21][200/469]\tTime 0.169 (0.170)\tData 0.007 (0.015)\tLoss 1.6783 (1.6416)\tPrec 78.125% (81.907%)\tPrec5 99.219% (99.258%)\nEpoch: [21][300/469]\tTime 0.162 (0.169)\tData 0.010 (0.015)\tLoss 1.6961 (1.6406)\tPrec 76.562% (82.026%)\tPrec5 99.219% (99.263%)\nEpoch: [21][400/469]\tTime 0.164 (0.169)\tData 0.013 (0.014)\tLoss 1.6191 (1.6401)\tPrec 84.375% (82.101%)\tPrec5 99.219% (99.217%)\nTest: [0/79]\tTime 0.436 (0.436)\tLoss 1.6352 (1.6352)\tPrec 82.812% (82.812%)\tPrec5 99.219% (99.219%)\n * Prec 81.620% \nTrain |Loss : 1.639178182220459|average accuracy Top-1 : 82.19166666259765|average accuracy Top-5 : 99.21333333333334|\nTrain |Loss : 1.6452107349395753 | average accuracy Top-1 : 81.62|average accuracy Top-5 : 99.26|\n*********************** Epoch : 21  ********************\nEpoch: [22][0/469]\tTime 0.438 (0.438)\tData 0.364 (0.364)\tLoss 1.6853 (1.6853)\tPrec 76.562% (76.562%)\tPrec5 98.438% (98.438%)\nEpoch: [22][100/469]\tTime 0.168 (0.171)\tData 0.007 (0.022)\tLoss 1.5852 (1.6447)\tPrec 88.281% (81.544%)\tPrec5 100.000% (99.118%)\nEpoch: [22][200/469]\tTime 0.168 (0.170)\tData 0.010 (0.017)\tLoss 1.6311 (1.6394)\tPrec 83.594% (82.160%)\tPrec5 99.219% (99.219%)\nEpoch: [22][300/469]\tTime 0.168 (0.169)\tData 0.008 (0.016)\tLoss 1.6234 (1.6386)\tPrec 82.812% (82.273%)\tPrec5 99.219% (99.245%)\nEpoch: [22][400/469]\tTime 0.163 (0.169)\tData 0.011 (0.015)\tLoss 1.6799 (1.6401)\tPrec 78.125% (82.103%)\tPrec5 98.438% (99.207%)\nTest: [0/79]\tTime 0.409 (0.409)\tLoss 1.6351 (1.6351)\tPrec 82.812% (82.812%)\tPrec5 99.219% (99.219%)\n * Prec 81.630% \nTrain |Loss : 1.6399257410685222|average accuracy Top-1 : 82.12333332926433|average accuracy Top-5 : 99.20166666666667|\nTrain |Loss : 1.6452071329116822 | average accuracy Top-1 : 81.63|average accuracy Top-5 : 99.26|\n*********************** Epoch : 22  ********************\nEpoch: [23][0/469]\tTime 0.427 (0.427)\tData 0.358 (0.358)\tLoss 1.6490 (1.6490)\tPrec 81.250% (81.250%)\tPrec5 99.219% (99.219%)\nEpoch: [23][100/469]\tTime 0.171 (0.170)\tData 0.017 (0.017)\tLoss 1.6242 (1.6432)\tPrec 82.812% (81.706%)\tPrec5 99.219% (99.134%)\nEpoch: [23][200/469]\tTime 0.167 (0.170)\tData 0.010 (0.016)\tLoss 1.6479 (1.6396)\tPrec 82.031% (82.125%)\tPrec5 100.000% (99.129%)\nEpoch: [23][300/469]\tTime 0.269 (0.170)\tData 0.179 (0.016)\tLoss 1.6346 (1.6397)\tPrec 82.031% (82.145%)\tPrec5 99.219% (99.156%)\nEpoch: [23][400/469]\tTime 0.164 (0.169)\tData 0.010 (0.015)\tLoss 1.6232 (1.6397)\tPrec 83.594% (82.152%)\tPrec5 100.000% (99.184%)\nTest: [0/79]\tTime 0.411 (0.411)\tLoss 1.6351 (1.6351)\tPrec 82.812% (82.812%)\tPrec5 99.219% (99.219%)\n * Prec 81.630% \nTrain |Loss : 1.6397399881362915|average accuracy Top-1 : 82.14333332519531|average accuracy Top-5 : 99.20333332926432|\nTrain |Loss : 1.6452078550338745 | average accuracy Top-1 : 81.63|average accuracy Top-5 : 99.26|\n*********************** Epoch : 23  ********************\nEpoch: [24][0/469]\tTime 0.430 (0.430)\tData 0.359 (0.359)\tLoss 1.6526 (1.6526)\tPrec 80.469% (80.469%)\tPrec5 97.656% (97.656%)\nEpoch: [24][100/469]\tTime 0.162 (0.171)\tData 0.008 (0.017)\tLoss 1.6287 (1.6455)\tPrec 82.812% (81.521%)\tPrec5 99.219% (99.103%)\nEpoch: [24][200/469]\tTime 0.166 (0.170)\tData 0.010 (0.015)\tLoss 1.6023 (1.6431)\tPrec 86.719% (81.829%)\tPrec5 100.000% (99.180%)\nEpoch: [24][300/469]\tTime 0.163 (0.169)\tData 0.009 (0.014)\tLoss 1.6920 (1.6415)\tPrec 77.344% (82.013%)\tPrec5 100.000% (99.188%)\nEpoch: [24][400/469]\tTime 0.164 (0.169)\tData 0.009 (0.015)\tLoss 1.6658 (1.6405)\tPrec 78.906% (82.125%)\tPrec5 99.219% (99.209%)\nTest: [0/79]\tTime 0.400 (0.400)\tLoss 1.6350 (1.6350)\tPrec 82.812% (82.812%)\tPrec5 99.219% (99.219%)\n * Prec 81.630% \nTrain |Loss : 1.6404748970667522|average accuracy Top-1 : 82.10166666259765|average accuracy Top-5 : 99.21|\nTrain |Loss : 1.6452050405502319 | average accuracy Top-1 : 81.63|average accuracy Top-5 : 99.26|\n*********************** Epoch : 24  ********************\nEpoch: [25][0/469]\tTime 0.443 (0.443)\tData 0.370 (0.370)\tLoss 1.6305 (1.6305)\tPrec 82.812% (82.812%)\tPrec5 98.438% (98.438%)\nEpoch: [25][100/469]\tTime 0.166 (0.171)\tData 0.010 (0.017)\tLoss 1.7121 (1.6372)\tPrec 74.219% (82.418%)\tPrec5 97.656% (99.134%)\nEpoch: [25][200/469]\tTime 0.166 (0.170)\tData 0.010 (0.015)\tLoss 1.6660 (1.6397)\tPrec 78.906% (82.113%)\tPrec5 97.656% (99.211%)\nEpoch: [25][300/469]\tTime 0.167 (0.169)\tData 0.012 (0.014)\tLoss 1.6330 (1.6404)\tPrec 82.031% (82.060%)\tPrec5 99.219% (99.128%)\nEpoch: [25][400/469]\tTime 0.163 (0.169)\tData 0.017 (0.014)\tLoss 1.6693 (1.6410)\tPrec 78.906% (82.000%)\tPrec5 100.000% (99.129%)\nTest: [0/79]\tTime 0.422 (0.422)\tLoss 1.6350 (1.6350)\tPrec 82.812% (82.812%)\tPrec5 99.219% (99.219%)\n * Prec 81.630% \nTrain |Loss : 1.6400340778986613|average accuracy Top-1 : 82.10999999593099|average accuracy Top-5 : 99.17999999593098|\nTrain |Loss : 1.6452047342300415 | average accuracy Top-1 : 81.63|average accuracy Top-5 : 99.26|\n*********************** Epoch : 25  ********************\nEpoch: [26][0/469]\tTime 0.411 (0.411)\tData 0.351 (0.351)\tLoss 1.6516 (1.6516)\tPrec 81.250% (81.250%)\tPrec5 100.000% (100.000%)\nEpoch: [26][100/469]\tTime 0.170 (0.173)\tData 0.011 (0.023)\tLoss 1.6400 (1.6389)\tPrec 82.031% (82.163%)\tPrec5 100.000% (99.327%)\nEpoch: [26][200/469]\tTime 0.174 (0.171)\tData 0.035 (0.020)\tLoss 1.7036 (1.6398)\tPrec 75.781% (82.136%)\tPrec5 96.875% (99.211%)\nEpoch: [26][300/469]\tTime 0.164 (0.170)\tData 0.023 (0.017)\tLoss 1.6106 (1.6387)\tPrec 85.156% (82.252%)\tPrec5 100.000% (99.240%)\n","output_type":"stream"}]},{"cell_type":"code","source":"plt.figure()\nplt.plot(learning_rates)\nplt.title(\"Learning Rate\")\nplt.ylabel(\"Learning Rate Value\")\nplt.xlabel(\"Epochs\")\nplt.show()\n\nplt.figure()\nplt.plot(train_loss_epoch)\nplt.plot(test_loss_epoch)\nplt.title(\"Train and Test Loss\")\nplt.ylabel(\"Loss\")\nplt.xlabel(\"Epochs\")\nplt.legend(['Train Loss', 'Test Loss'])\nplt.show()\n\nplt.figure()\nplt.plot(acc_train_1)\nplt.plot(acc_test_1)\nplt.title(\"Train and Test Accuracy\")\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"Epochs\")\nplt.legend(['Train Accuracy', 'Test Accuracy'])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_results = output_results(train_loader, best_val_model, num_classes, data_name='Train')\ntest_results = output_results(test_loader, best_val_model, num_classes, data_name='Test')\nfinal_result = pd.concat([train_results, test_results], axis=1)\nprint(final_result)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}